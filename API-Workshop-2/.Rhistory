karen_top_terms2 <-
karen_topics2 %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
karen_top_terms2 %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
karen_comments_regex <- karen_subreddit_data %>%
select( comm_date, comment ) %>%
filter( grepl( "karen" , comment, ignore.case = TRUE ) )
#Creating a corpus
karen_comment_corpus <- Corpus(VectorSource(as.vector(karen_comments_regex$comment)))
head(karen_comment_corpus)
#Remove stop words
karen_comment_corpus <- tm_map(karen_comment_corpus, removeWords, stopwords("english"))
#Remove punctuation
karen_comment_corpus <- tm_map(karen_comment_corpus, content_transformer(removePunctuation))
#Remove numbers
karen_comment_corpus <- tm_map(karen_comment_corpus, content_transformer(removeNumbers))
#To lower case
karen_comment_corpus <- tm_map(karen_comment_corpus,  content_transformer(tolower))
#Remove whitespace
karen_comment_corpus <- tm_map(karen_comment_corpus, content_transformer(stripWhitespace))
#Check english language
karen_comment_corpus  <- tm_map(karen_comment_corpus, content_transformer(stemDocument), language = "english")
#Document-Term Matrix
karen_comment_dtm <- DocumentTermMatrix(karen_comment_corpus, control = list(wordLengths = c(2, Inf)))
inspect(karen_comment_dtm[1:5,3:8])
karen_topic_model2<-LDA(karen_comment_dtm, k=10, control = list(seed = 321))
karen_topics2 <- tidy(karen_topic_model2, matrix = "beta")
karen_top_terms2 <-
karen_topics2 %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
karen_top_terms2 %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
View(karen_comment_dtm)
View(karen_comment_dtm)
# Remove Karen, so it doesn't overflow the topic model
karen_comment_corpus <- tm_map(karen_comment_corpus, removeWords, c("karen","karens"))
karen_comment_corpus <- Corpus(VectorSource(as.vector(karen_comments_regex$comment)))
head(karen_comment_corpus)
#Remove stop words
karen_comment_corpus <- tm_map(karen_comment_corpus, removeWords, stopwords("english"))
#Remove punctuation
karen_comment_corpus <- tm_map(karen_comment_corpus, content_transformer(removePunctuation))
#Remove numbers
karen_comment_corpus <- tm_map(karen_comment_corpus, content_transformer(removeNumbers))
#To lower case
karen_comment_corpus <- tm_map(karen_comment_corpus,  content_transformer(tolower))
#Remove whitespace
karen_comment_corpus <- tm_map(karen_comment_corpus, content_transformer(stripWhitespace))
#Check english language
karen_comment_corpus  <- tm_map(karen_comment_corpus, content_transformer(stemDocument), language = "english")
# Remove Karen, so it doesn't overflow the topic model
karen_comment_corpus <- tm_map(karen_comment_corpus, removeWords, c("karen","karens"))
#Document-Term Matrix
karen_comment_dtm <- DocumentTermMatrix(karen_comment_corpus, control = list(wordLengths = c(2, Inf)))
karen_topic_model2<-LDA(karen_comment_dtm, k=10, control = list(seed = 321))
# karen_dtm_noempty <- karen_comment_dtm[ which(apply(karen_comment_dtm , 1, sum) > 0), ]
karen_dtm_noempty2 <- karen_comment_dtm[ which(apply(karen_comment_dtm , 1, sum) > 0), ]
karen_comment_corpus <- Corpus(VectorSource(as.vector(karen_comments_regex$comment)))
head(karen_comment_corpus)
#Remove stop words
karen_comment_corpus <- tm_map(karen_comment_corpus, removeWords, stopwords("english"))
#Remove punctuation
karen_comment_corpus <- tm_map(karen_comment_corpus, content_transformer(removePunctuation))
#Remove numbers
karen_comment_corpus <- tm_map(karen_comment_corpus, content_transformer(removeNumbers))
#To lower case
karen_comment_corpus <- tm_map(karen_comment_corpus,  content_transformer(tolower))
#Remove whitespace
karen_comment_corpus <- tm_map(karen_comment_corpus, content_transformer(stripWhitespace))
#Check english language
karen_comment_corpus  <- tm_map(karen_comment_corpus, content_transformer(stemDocument), language = "english")
# Remove Karen, so it doesn't overflow the topic model
# Caused a significant error
# karen_comment_corpus <- tm_map(karen_comment_corpus, removeWords, c("karen","karens"))
#Document-Term Matrix
karen_comment_dtm <- DocumentTermMatrix(karen_comment_corpus, control = list(wordLengths = c(2, Inf)))
karen_topic_model2<-LDA(karen_comment_dtm, k=10, control = list(seed = 321))
karen_topics2 <- tidy(karen_topic_model2, matrix = "beta")
karen_top_terms2 <-
karen_topics2 %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
karen_top_terms2 %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
ggplot(top_20, aes(x=word, y=n, fill=word))+
geom_bar(stat="identity")+
theme_minimal()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))+
ylab("Number of Times Word Appears in Reddit Comments that Contain: Karen ")+
xlab("")+
guides(fill=FALSE)
# Basic GREP for Karen within larger dataset
karen_comments <- karen_subreddit_data %>%
select( comm_date, comment ) %>%
filter( grepl( "karen", comment, ignore.case = TRUE ) )
# Isolate the comment post date and comment text
# Regular expression captures repeat characters
karen_comments_regex <- karen_subreddit_data %>%
select( comm_date, comment ) %>%
filter( grepl( "karen" , comment, ignore.case = TRUE ) )
# Tokenize comments
tidy_karen_comments<- karen_comments_regex %>%
select(comm_date, comment ) %>%
unnest_tokens("word", comment)
# A set of standard builtin stop words
data("stop_words")
# Anti-join to remove stop words from tokenized data
# Result is an ordered list of the top words used in the comments
karen_comment_top_words<-
tidy_karen_comments %>%
anti_join(stop_words) %>%
count(word) %>%
arrange(desc(n))
# Remove platform-specific stop words
karen_comment_top_words<-
karen_comment_top_words[-grep("https|t.co|amp|rt|karen",
karen_comment_top_words$word),]
# Plot of top words
top_20<-karen_comment_top_words[1:20,]
#create factor variable to sort by frequency
karen_comment_top_words$word <- factor(karen_comment_top_words$word, levels = karen_comment_top_words$word[order(karen_comment_top_words$n,decreasing=TRUE)])
# Plot code
ggplot(top_20, aes(x=word, y=n, fill=word))+
geom_bar(stat="identity")+
theme_minimal()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))+
ylab("Number of Times Word Appears in Reddit Comments that Contain: Karen ")+
xlab("")+
guides(fill=FALSE)
library( tidytext )
library( tidyverse )
library( dplyr )
library( stringr )
# --------------- Dictionary Analysis ---------------------#
# Basic GREP for Karen within larger dataset
karen_comments <- karen_subreddit_data %>%
select( comm_date, comment ) %>%
filter( grepl( "karen", comment, ignore.case = TRUE ) )
# Isolate the comment post date and comment text
# Regular expression captures repeat characters
karen_comments_regex <- karen_subreddit_data %>%
select( comm_date, comment ) %>%
filter( grepl( "karen" , comment, ignore.case = TRUE ) )
# Tokenize comments
tidy_karen_comments<- karen_comments_regex %>%
select(comm_date, comment ) %>%
unnest_tokens("word", comment)
# A set of standard builtin stop words
data("stop_words")
# Anti-join to remove stop words from tokenized data
# Result is an ordered list of the top words used in the comments
karen_comment_top_words<-
tidy_karen_comments %>%
anti_join(stop_words) %>%
count(word) %>%
arrange(desc(n))
# Remove platform-specific stop words
karen_comment_top_words<-
karen_comment_top_words[-grep("https|t.co|amp|rt|karen",
karen_comment_top_words$word),]
# Plot of top words
top_20<-karen_comment_top_words[1:20,]
#create factor variable to sort by frequency
karen_comment_top_words$word <- factor(karen_comment_top_words$word, levels = karen_comment_top_words$word[order(karen_comment_top_words$n,decreasing=TRUE)])
# Plot code
ggplot(top_20, aes(x=word, y=n, fill=word))+
geom_bar(stat="identity")+
theme_minimal()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))+
ylab("Number of Times Word Appears in Reddit Comments that Contain: Karen ")+
xlab("")+
guides(fill=FALSE)
karen_comment_top_words<-
karen_comment_top_words[-grep("https|t.co|amp|rt|karen|people",
karen_comment_top_words$word),]
# Plot of top words
top_20<-karen_comment_top_words[1:20,]
#create factor variable to sort by frequency
karen_comment_top_words$word <- factor(karen_comment_top_words$word, levels = karen_comment_top_words$word[order(karen_comment_top_words$n,decreasing=TRUE)])
# Plot code
ggplot(top_20, aes(x=word, y=n, fill=word))+
geom_bar(stat="identity")+
theme_minimal()+
theme(axis.text.x = element_text(angle = 90, hjust = 1))+
ylab("Number of Times Word Appears in Reddit Comments that Contain: Karen ")+
xlab("")+
guides(fill=FALSE)
# Implementing term frequency inverse document frequency (tf-idf) to capture missed stop words specific to this dataset
tidy_karen_tfidf<- karen_comments_regex %>%
select(comm_date, comment) %>%
unnest_tokens("word", comment) %>%
anti_join(stop_words) %>%
count(word, comm_date) %>%
bind_tf_idf(word, comm_date, n)
# Print out the top words
top_tfidf<-tidy_karen_tfidf %>%
arrange(desc(tf_idf))
top_tfidf$word[1]
karen_comment_sentiment <- tidy_karen_comments %>%
inner_join(get_sentiments("bing")) %>%
count(comm_date, sentiment)
head(karen_comment_sentiment)
# Visual based on the date of the comments
tidy_karen_comments$date<-as.Date(tidy_karen_comments$comm_date,
format="%Y-%m-%d %x")
# Plot of sentiment over time
karen_sentiment_plot <-
tidy_karen_comments %>%
inner_join(get_sentiments("bing")) %>%
filter( sentiment == "negative" ) %>%
count(date, sentiment)
ggplot(karen_sentiment_plot, aes(x=date, y=n))+
geom_line(color="red")+
theme_minimal()+
ylab("Frequency of Negative Words in Reddit's Karen Comments")+
xlab("Date")
install.packages("tuber")
yt_oauth("31827417194-ri7ng7knjaf7e6ugt7lnt7k2vo94fkci.apps.googleusercontent.com", "4SXjDMvac99DPRgb7vPMZLTY")
library(tuber)
yt_oauth("31827417194-ri7ng7knjaf7e6ugt7lnt7k2vo94fkci.apps.googleusercontent.com", "4SXjDMvac99DPRgb7vPMZLTY")
yt_oauth("31827417194-ri7ng7knjaf7e6ugt7lnt7k2vo94fkci.apps.googleusercontent.com", "4SXjDMvac99DPRgb7vPMZLTY")
library(tuber)
yt_oauth("31827417194-ri7ng7knjaf7e6ugt7lnt7k2vo94fkci.apps.googleusercontent.com", "4SXjDMvac99DPRgb7vPMZLTY")
library(tuber)
yt_oauth("31827417194-ri7ng7knjaf7e6ugt7lnt7k2vo94fkci.apps.googleusercontent.com", "4SXjDMvac99DPRgb7vPMZLTY")
library(tuber)
file.remove(".httr-oauth")
yt_oauth("31827417194-ri7ng7knjaf7e6ugt7lnt7k2vo94fkci.apps.googleusercontent.com", "4SXjDMvac99DPRgb7vPMZLTY")
file.remove(".httr-oauth")
yt_oauth("31827417194-ri7ng7knjaf7e6ugt7lnt7k2vo94fkci.apps.googleusercontent.com", "4SXjDMvac99DPRgb7vPMZLTY")
file.remove(".httr-oauth")
yt_oauth("31827417194-cia1c77vh987roh9tjpt6phligjn67ug.apps.googleusercontent.com", "WUvUvhnvkZ6_6XFNV0LbtFT-")
yt_oauth("31827417194-cia1c77vh987roh9tjpt6phligjn67ug.apps.googleusercontent.com", "WUvUvhnvkZ6_6XFNV0LbtFT-")
library(tuber)
yt_oauth("31827417194-cia1c77vh987roh9tjpt6phligjn67ug.apps.googleusercontent.com", "WUvUvhnvkZ6_6XFNV0LbtFT-")
library(tuber)
file.remove(".httr-oauth")
yt_oauth("31827417194-cia1c77vh987roh9tjpt6phligjn67ug.apps.googleusercontent.com", "WUvUvhnvkZ6_6XFNV0LbtFT-")
library(tuber)
file.remove(".httr-oauth")
yt_oauth("31827417194-cia1c77vh987roh9tjpt6phligjn67ug.apps.googleusercontent.com", "WUvUvhnvkZ6_6XFNV0LbtFT-")
file.remove(".httr-oauth")
yt_oauth("31827417194-cia1c77vh987roh9tjpt6phligjn67ug.apps.googleusercontent.com", "WUvUvhnvkZ6_6XFNV0LbtFT-", token = "")
library(tuber)
file.remove(".httr-oauth")
library(tuber)
file.remove(".httr-oauth")
yt_oauth("31827417194-5b946nko9npq33mdk7ubacrpk85me04u.apps.googleusercontent.com", "ca8nqfPybOvnuu9NFkJ7_kdT", token = "")
library(tuber)
file.remove(".httr-oauth")
yt_oauth("31827417194-5b946nko9npq33mdk7ubacrpk85me04u.apps.googleusercontent.com", "ca8nqfPybOvnuu9NFkJ7_kdT", token = "")
file.remove(".httr-oauth")
yt_oauth("31827417194-5b946nko9npq33mdk7ubacrpk85me04u.apps.googleusercontent.com", "ca8nqfPybOvnuu9NFkJ7_kdT", token = "")
file.remove(".httr-oauth")
yt_oauth("31827417194-cia1c77vh987roh9tjpt6phligjn67ug.apps.googleusercontent.com", "WUvUvhnvkZ6_6XFNV0LbtFT-", token = "")
library(tuber)
file.remove(".httr-oauth")
yt_oauth("31827417194-5b946nko9npq33mdk7ubacrpk85me04u.apps.googleusercontent.com", "ca8nqfPybOvnuu9NFkJ7_kdT")
library(tuber)
# Clears the oauth file. If you don't clear this and you hit an error, then you can't try again to authenticate.
file.remove(".httr-oauth")
# DESKTOP APP - AUTH
yt_oauth("31827417194-cia1c77vh987roh9tjpt6phligjn67ug.apps.googleusercontent.com", "WUvUvhnvkZ6_6XFNV0LbtFT-", token = "")
# WEB APP - AUTH
# yt_oauth("31827417194-5b946nko9npq33mdk7ubacrpk85me04u.apps.googleusercontent.com", "ca8nqfPybOvnuu9NFkJ7_kdT")
library(tuber)
# Clears the oauth file. If you don't clear this and you hit an error, then you can't try again to authenticate.
file.remove(".httr-oauth")
# DESKTOP APP - AUTH
yt_oauth("31827417194-cia1c77vh987roh9tjpt6phligjn67ug.apps.googleusercontent.com", "WUvUvhnvkZ6_6XFNV0LbtFT-", token = "")
# Clears the oauth file. If you don't clear this and you hit an error, then you can't try again to authenticate.
file.remove(".httr-oauth")
# DESKTOP APP - AUTH
yt_oauth("31827417194-cia1c77vh987roh9tjpt6phligjn67ug.apps.googleusercontent.com", "WUvUvhnvkZ6_6XFNV0LbtFT-", token = "")
get_stats(video_id="N708P-A45D0")
res <- get_comment_threads(c(video_id="N708P-A45D0"))
head(res)
View(res)
load("~/PhD/SICSS/Final Project/subreddits_with_karen_discourse.RData")
knitr::opts_chunk$set(echo = TRUE)
# Installs the academictwitteR package from CRAN
# install.packages("academictwitteR")
library(academictwitteR)
# Set Bearer Token function from the academictwitteR package.
# set_bearer()
setwd("~/PhD/Projects/CCSS/API-workshops/API-Workshop-2")
knitr::opts_chunk$set(echo = TRUE)
# Installs the academictwitteR package from CRAN
# install.packages("academictwitteR")
library(academictwitteR)
# Set Bearer Token function from the academictwitteR package.
# set_bearer()
# Timezone: https://developer.twitter.com/en/docs/twitter-ads-api/timezones
get_all_tweets(
query = "Saint Patrick's Day", # keyword search (what are some keywords that align with important dates?)
start_tweets = "2021-03-16T00:00:00Z",
end_tweets = "2021-03-18T00:00:00Z",
n = 100, #how many tweets do you want?
#bearer_token = "",  either set this earlier or you will have to put it down every time
data_path = "example", # If you want to store the output on your computer, put a path here
bind_tweets = FALSE, # More on this below
)
# Here we change the format of the output of the tweets into something that is useable in R
stpat_tweetsraw <- bind_tweets(data_path = "example", output_format = "raw")
class(stpat_tweetsraw)
# Let's take a look at the variables that we have access to from our Twitter data pull
bind_tweets(data_path = "example", output_format = "raw") %>%
names
# Timezone: https://developer.twitter.com/en/docs/twitter-ads-api/timezones
get_all_tweets(
query = "Saint Patrick's Day", # keyword search (what are some keywords that align with important dates?)
start_tweets = "2021-03-16T00:00:00Z",
end_tweets = "2021-03-18T00:00:00Z",
n = 100, #how many tweets do you want?
#bearer_token = "",  either set this earlier or you will have to put it down every time
data_path = "example", # If you want to store the output on your computer, put a path here
bind_tweets = FALSE, # More on this below
)
# Here we change the format of the output of the tweets into something that is useable in R
stpat_tweetsraw <- bind_tweets(data_path = "example", output_format = "raw")
class(stpat_tweetsraw)
# Let's take a look at the variables that we have access to from our Twitter data pull
bind_tweets(data_path = "example", output_format = "raw") %>%
names
# Here is the "tidy" version of the data:
stpat_tidy <- bind_tweets(data_path = "example", output_format = "tidy")
class(stpat_tidy)
# More readable, right?
head(stpat_tidy, 5)
count_all_tweets(
query = "tokyo", #Perhaps a hashtag analysis? What other keywords would be useful for this search method?
start_tweets = "2021-07-15T00:00:00Z",
end_tweets = "2021-08-01T00:00:00Z",
bearer_token = "",
granularity="day")
count_all_tweets(
query = "tokyo", #Perhaps a hashtag analysis? What other keywords would be useful for this search method?
start_tweets = "2021-07-15T00:00:00Z",
end_tweets = "2021-08-01T00:00:00Z",
# bearer_token = "",
granularity="day")
profile_tweets <-
get_all_tweets(
users = c("InfoTisayentist"),
start_tweets = "2021-01-01T00:00:00Z",
end_tweets = "2021-01-05T00:00:00Z",
# bearer_token = "",
n = 100
)
head(profile_tweets$text, n=5)
thread_tweets <-
get_all_tweets(
# Replace with Tweet ID of your choice to get replies (this ID is from the St Pat original set)
conversation_id = "1372265434045681666",
start_tweets = "2021-03-01T00:00:00Z",
end_tweets = "2021-03-30T00:00:00Z",
bind_tweets = TRUE,
n = 100,
bearer_token = "",
)
thread_tweets <-
get_all_tweets(
# Replace with Tweet ID of your choice to get replies (this ID is from the St Pat original set)
conversation_id = "1372265434045681666",
start_tweets = "2021-03-01T00:00:00Z",
end_tweets = "2021-03-30T00:00:00Z",
bind_tweets = TRUE,
n = 100,
# bearer_token = "",
)
head(thread_tweets$text, n=4 )
ex_geo <-
get_all_tweets(
query = "Rams", # Perhaps something more geo-dependent?
start_tweets = "2022-2-12T00:00:00Z",
end_tweets = "2022-02-14T00:00:00Z",
n = 100, #set an upper limit. The default is 100.
has_geo = TRUE,
# bearer_token = "",
)
head(ex_geo$text, n=5)
# Replace with author IDs of your choice - these are also from the St Pat query.
ex_userids <- c("726782599381815296", "73533164")
ex_user_meta <-
get_user_profile(
ex_userids)
head(ex_user_meta, n=5)
adv_tweets <-
get_all_tweets(
query = "Rams", # Perhaps something more geo-dependent?
start_tweets = "2022-2-12T00:00:00Z",
end_tweets = "2022-02-14T00:00:00Z",
n = 100, #set an upper limit. The default is 100.
country = "US", is_verified = FALSE,
lang = "en",  is_retweet = FALSE,
has_geo = TRUE,
# bearer_token = "",
)
head(adv_tweets$text, n=5)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
# Installs the academictwitteR package from CRAN
# install.packages("academictwitteR")
library(academictwitteR)
# Set Bearer Token function from the academictwitteR package.
# set_bearer()
# Timezone: https://developer.twitter.com/en/docs/twitter-ads-api/timezones
get_all_tweets(
query = "Saint Patrick's Day", # keyword search (what are some keywords that align with important dates?)
start_tweets = "2021-03-16T00:00:00Z",
end_tweets = "2021-03-18T00:00:00Z",
n = 100, #how many tweets do you want?
#bearer_token = "",  either set this earlier or you will have to put it down every time
data_path = "example", # If you want to store the output on your computer, put a path here
bind_tweets = FALSE, # More on this below
)
# Timezone: https://developer.twitter.com/en/docs/twitter-ads-api/timezones
get_all_tweets(
query = "Saint Patrick's Day", # keyword search (what are some keywords that align with important dates?)
start_tweets = "2021-03-16T00:00:00Z",
end_tweets = "2021-03-18T00:00:00Z",
n = 100, #how many tweets do you want?
#bearer_token = "",  either set this earlier or you will have to put it down every time
data_path = "example", # If you want to store the output on your computer, put a path here
bind_tweets = FALSE, # More on this below
)
# Here we change the format of the output of the tweets into something that is useable in R
stpat_tweetsraw <- bind_tweets(data_path = "example", output_format = "raw")
class(stpat_tweetsraw)
# Let's take a look at the variables that we have access to from our Twitter data pull
bind_tweets(data_path = "example", output_format = "raw") %>%
names
# Here is the "tidy" version of the data:
stpat_tidy <- bind_tweets(data_path = "example", output_format = "tidy")
class(stpat_tidy)
# More readable, right?
head(stpat_tidy, 5)
count_all_tweets(
query = "tokyo", #Perhaps a hashtag analysis? What other keywords would be useful for this search method?
start_tweets = "2021-07-15T00:00:00Z",
end_tweets = "2021-08-01T00:00:00Z",
# bearer_token = "",
granularity="day")
profile_tweets <-
get_all_tweets(
users = c("InfoTisayentist"),
start_tweets = "2021-01-01T00:00:00Z",
end_tweets = "2021-01-05T00:00:00Z",
# bearer_token = "",
n = 100
)
head(profile_tweets$text, n=5)
thread_tweets <-
get_all_tweets(
# Replace with Tweet ID of your choice to get replies (this ID is from the St Pat original set)
conversation_id = "1372265434045681666",
start_tweets = "2021-03-01T00:00:00Z",
end_tweets = "2021-03-30T00:00:00Z",
bind_tweets = TRUE,
n = 100,
# bearer_token = "",
)
head(thread_tweets$text, n=4 )
ex_geo <-
get_all_tweets(
query = "Rams", # Perhaps something more geo-dependent?
start_tweets = "2022-2-12T00:00:00Z",
end_tweets = "2022-02-14T00:00:00Z",
n = 100, #set an upper limit. The default is 100.
has_geo = TRUE,
# bearer_token = "",
)
head(ex_geo$text, n=5)
# Replace with author IDs of your choice - these are also from the St Pat query.
ex_userids <- c("726782599381815296", "73533164")
ex_user_meta <-
get_user_profile(
ex_userids)
head(ex_user_meta, n=5)
adv_tweets <-
get_all_tweets(
query = "Rams", # Perhaps something more geo-dependent?
start_tweets = "2022-2-12T00:00:00Z",
end_tweets = "2022-02-14T00:00:00Z",
n = 100, #set an upper limit. The default is 100.
country = "US", is_verified = FALSE,
lang = "en",  is_retweet = FALSE,
has_geo = TRUE,
# bearer_token = "",
)
head(adv_tweets$text, n=5)
